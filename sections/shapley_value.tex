\begin{frame}{Shapley Value - Intuition}
	\begin{itemize}
		\item The average marginal contribution of a feature value over all possible coalitions.
		\item Shapley value for a feature $j$: average change in prediction that a subset of features receives when the feature $j$ joins them.
	\end{itemize}
\end{frame}
\begin{frame}{Shapley Value - Feature Contribution}
\begin{equation}
	\phi_j(val_{\bm{x}})=\frac{1}{p}\sum_{S\subseteq\{1,\ldots,p\} \backslash \{j\}}\binom{p-1}{|S|}^{-1}\Bigl(val_{\bm{x}}\bigl(S\cup\{j\}\bigr)-val_{\bm{x}}(S)\Bigr)
\end{equation}
\begin{itemize}
	\item Contribution of $j$-th feature value to the prediction (\emph{payout})
	\item Normalized: weighted and summed over all possible feature combinations
	\item $j$-th feature value
	\item $val_{\bm{x}}(S)$: value of players in $S$ %\red{to explain $\bm{x}$}
	\item $S$: a subset of features used in the model (\emph{coalition})
	\item $\bm{x}$: vector of feature values of an instance to be explained
	\item $p$: nr. features
\end{itemize}
\end{frame}

\input{sections/shapley_value_function.tex}

\begin{frame}{Shapley Value - Feature Contribution - Example}
	\begin{itemize}
		\item Prediction function: $\hat{f}(x_1, x_2) = x_1 \lor x_2$
		\item $x_1, x_2 \sim U(\{0, 1\})$, $E_{X_1,X_2}(\hat{f}(X_1,X_2)) = \frac{3}{4}$
		\item E.g., input: $x_1 = 1$, $x_2 = 1$
		\item Prediction: $\hat{f}(1, 1) = 1$
		\item Gain: $\hat{f}(1, 1) - E_{X_1,X_2}(\hat{f}(X_1,X_2)) = 1 - \frac{3}{4} = \frac{1}{4}$
		\item Contribution of feature value $x_1 = 1$
		\begin{align}\begin{split}
				\phi_1(val_{\bm{x}})
				&=\frac{1}{2} \Bigl[\bigl(val_{\bm{x}}\left(\{1\}\right)-val_{\bm{x}}\left(\{\}\right)\bigr) + \bigl(val_{\bm{x}}\left(\{1, 2\}\right)-val_{\bm{x}}\left(\{2\}\right)\bigr)\Bigr]\\
				&=\frac{1}{2} val_{\bm{x}}\left(\{1, 2\}\right)\\
				&=\frac{1}{2} \Bigl[\hat{f}(1, 1) - E_{X_1,X_2}\left(\hat{f}(X_1,X_2)\right)\Bigr]\\
				&=\frac{1}{2} \left[1 - \frac{3}{4}\right] = \frac{1}{8}
		\end{split}\end{align}
		\vspace{-2em}
		\begin{itemize}
			\item Perturbs all subsets of features
		\end{itemize}
		\item Same contribution for feature value $x_2 = 1$
	\end{itemize}
\end{frame}

\input{sections/shapley_value_properties.tex}

\begin{frame}{Shapley Value - Exact Estimation}
	\begin{itemize}
		\item All possible subsets (coalitions) of feature values have to be evaluated with and without the $j$-th feature.
		\item The number of possible coalitions increases exponentially as the the number of features increases.
	\end{itemize}
\end{frame}
\begin{frame}{Shapley Value - Approximation\footnotemark}
\begin{equation}
	\hat{\phi}_{j}=\frac{1}{M}\sum_{m=1}^M\left(\hat{f}(\bm{x}^{m}_{+j})-\hat{f}(\bm{x}^{m}_{-j})\right)
\end{equation}
\begin{itemize}
	\item Monte-Carlo Sampling
	\item $\hat{f}(\bm{x}^{m}_{+j})$
	\begin{itemize}
		\item prediction for a data point $\bm{x}^m$
		\item random number of features replaced by feature values from a random data point $\bm{z}^m$.
		\item uses the feature value $x^m_j$
	\end{itemize}
	\item $\hat{f}(\bm{x}^{m}_{-j})$
	\begin{itemize}
		\item like $\hat{f}(\bm{x}^{m}_{+j})$
		\item uses the random feature value $z^m_j$
	\end{itemize}
\end{itemize}
\footnotetext[4]{\citeauthor{vstrumbelj2014explaining} (\citeyear{vstrumbelj2014explaining})}
\end{frame}