\begin{frame}{Shapley Value - Intuition}
	\begin{itemize}\setlength\itemsep{4em}
		\item<1-> The average marginal contribution of a feature value over all possible coalitions.
		\item<2-> Shapley value for a feature $j$: average change in prediction that a subset of features receives when the feature $j$ joins them.
	\end{itemize}
\end{frame}
\begin{frame}{Shapley Value - Feature Contribution}
\begin{equation}
	\phi_j(\bm{x})=\frac{1}{p}\sum_{S\subseteq\{1,\ldots,p\} \backslash \{j\}}\binom{p-1}{|S|}^{-1}\left(val_{S\cup\{j\}}(\bm{x})-val_S(\bm{x})\right)
\end{equation}
\begin{itemize}\setlength\itemsep{2em}
	\item[]
	where
	\begin{itemize}
		\item $j$-th feature value
		\item $val_{\bm{x}}(S)$: value of players in $S$ %\red{to explain $\bm{x}$}
		\item $S$: a subset of features used in the model (\emph{coalition})
		\item $\bm{x}$: vector of feature values of an instance to be explained
		\item $p$: nr. features
	\end{itemize}
	\item<1-> Contribution of $j$-th feature value to the prediction (\emph{payout})
	\item<2-> Normalized: weighted and summed over all possible feature combinations
\end{itemize}
\end{frame}

\input{sections/shapley_value_function.tex}

\begin{frame}[shrink=3]{Shapley Value - Feature Contribution - Example}
	\begin{itemize}
		\item<1-> Prediction function: $\hat{f}(x_1, x_2) = x_1 \lor x_2$
		\item<1-> $x_1, x_2 \sim U(\{0, 1\})$, $\mathbb{E}_{X_1,X_2}\left[\hat{f}(X_1,X_2)\right] = \frac{3}{4}$
		\vspace{1em}
		\item<2-> E.g., input: $x_1 = 1$, $x_2 = 1$
		\item<2-> Prediction: $\hat{f}(1, 1) = 1$
		\item<2-> Gain: $\hat{f}(1, 1) - \mathbb{E}_{X_1,X_2}\left[\hat{f}(X_1,X_2)\right] = 1 - \frac{3}{4} = \frac{1}{4}$
		\vspace{1em}
		\item<3-> Contribution of feature value $x_1 = 1$
		\begin{align}\begin{split}
				\phi_1\left(x_1, x_2;\hat{f}\right)
				&=\frac{1}{2} \biggl[val_{\{1\}}\left(\bm{x};\hat{f}\right)-val_{\{\}}\left(\bm{x};\hat{f}\right)\\
				&\;\;\;\;\;\;\;\;+val_{\{1,2\}}\left(\bm{x};\hat{f}\right)-val_{\{2\}}\left(\bm{x};\hat{f}\right)\biggr]\\
				&=\frac{1}{2} val_{\{1,2\}}\left(\bm{x};\hat{f}\right)\\
				&=\frac{1}{2} \biggl(\hat{f}(1, 1) - \mathbb{E}_{X_1,X_2}\left[\hat{f}(X_1,X_2)\right]\biggr)
				=\frac{1}{2} \left[1 - \frac{3}{4}\right] = \frac{1}{8}
		\end{split}\end{align}
		\vspace{-0.5em}
		\begin{itemize}
			\item<3-> Perturbs all subsets of features
		\end{itemize}
		\item<4-> Same contribution for feature value $x_2 = 1$
	\end{itemize}
\end{frame}

\begin{frame}{Shapley Value - Exact Estimation}
	\begin{itemize}\setlength\itemsep{4em}
		\item All possible subsets (coalitions) of feature values have to be evaluated with and without the $j$-th feature.
		\item The number of possible coalitions increases exponentially as the the number of features increases.
	\end{itemize}
\end{frame}
\begin{frame}{Shapley Value - Approximation\footnotemark}
\begin{equation}
	\hat{\phi}_{j}\left(\bm{x};\hat{f}\right)=\frac{1}{M}\sum_{m=1}^M\left(\hat{f}\left(\bm{x}^{m}_{+j}\right)-\hat{f}\left(\bm{x}^{m}_{-j}\right)\right)
\end{equation}
\begin{itemize}
	\item Monte-Carlo Sampling
	\item $\hat{f}\left(\bm{x}^{m}_{+j}\right)$
	\begin{itemize}
		\item prediction for a data point $\bm{x}^m$
		\item random number of features replaced by feature values from a random data point $\bm{z}^m$.
		\item uses the feature value $x^m_j$
	\end{itemize}
	\item $\hat{f}\left(\bm{x}^{m}_{-j}\right)$
	\begin{itemize}
		\item like $\hat{f}\left(\bm{x}^{m}_{+j}\right)$
		\item uses the random feature value $z^m_j$
	\end{itemize}
\end{itemize}
\footnotetext[4]{\citeauthor{vstrumbelj2014explaining} (\citeyear{vstrumbelj2014explaining})}
\end{frame}